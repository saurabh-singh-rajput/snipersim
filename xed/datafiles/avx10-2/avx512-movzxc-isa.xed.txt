#BEGIN_LEGAL
#
#Copyright (c) 2024 Intel Corporation
#
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#  
#END_LEGAL
#
#
#
#    ***** GENERATED FILE -- DO NOT EDIT! *****
#    ***** GENERATED FILE -- DO NOT EDIT! *****
#    ***** GENERATED FILE -- DO NOT EDIT! *****
#
#
#
EVEX_INSTRUCTIONS()::
# EMITTING VMOVD (VMOVD-128-3)
{
ICLASS:      VMOVD
CPL:         3
CATEGORY:    DATAXFER
EXTENSION:   AVX512EVEX
ISA_SET:     AVX512_MOVZXC_128
EXCEPTIONS:  AVX512-E9NF
REAL_OPCODE: Y
PATTERN:     EVV 0x7E VF3 V0F MOD[0b11] MOD=3 BCRC=0 UBIT=1 REG[rrr] RM[nnn] W0 VL128 NOEVSR ZEROING=0 MASK=0
OPERANDS:    REG0=XMM_R3():w:dq:u32 REG1=XMM_B3():r:dq:u32
IFORM:       VMOVD_XMMu32_XMMu32_AVX512_MOVZXC
}

{
ICLASS:      VMOVD
CPL:         3
CATEGORY:    DATAXFER
EXTENSION:   AVX512EVEX
ISA_SET:     AVX512_MOVZXC_128
EXCEPTIONS:  AVX512-E9NF
REAL_OPCODE: Y
ATTRIBUTES:  DISP8_TUPLE1 
PATTERN:     EVV 0x7E VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() BCRC=0 UBIT=1 W0 VL128 NOEVSR ZEROING=0 MASK=0 ESIZE_32_BITS() NELEM_TUPLE1()
OPERANDS:    REG0=XMM_R3():w:dq:u32 MEM0:r:d:u32
IFORM:       VMOVD_XMMu32_MEMu32_AVX512_MOVZXC
}


# EMITTING VMOVD (VMOVD-128-4)
{
ICLASS:      VMOVD
CPL:         3
CATEGORY:    DATAXFER
EXTENSION:   AVX512EVEX
ISA_SET:     AVX512_MOVZXC_128
EXCEPTIONS:  AVX512-E9NF
REAL_OPCODE: Y
PATTERN:     EVV 0xD6 V66 V0F MOD[0b11] MOD=3 BCRC=0 UBIT=1 REG[rrr] RM[nnn] W0 VL128 NOEVSR ZEROING=0 MASK=0
OPERANDS:    REG0=XMM_B3():w:dq:u32 REG1=XMM_R3():r:dq:u32
IFORM:       VMOVD_XMMu32_XMMu32_AVX512_MOVZXC
}

{
ICLASS:      VMOVD
CPL:         3
CATEGORY:    DATAXFER
EXTENSION:   AVX512EVEX
ISA_SET:     AVX512_MOVZXC_128
EXCEPTIONS:  AVX512-E9NF
REAL_OPCODE: Y
ATTRIBUTES:  DISP8_TUPLE1 
PATTERN:     EVV 0xD6 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() BCRC=0 UBIT=1 W0 VL128 NOEVSR ZEROING=0 MASK=0 ESIZE_32_BITS() NELEM_TUPLE1()
OPERANDS:    MEM0:w:d:u32 REG0=XMM_R3():r:dq:u32
IFORM:       VMOVD_MEMu32_XMMu32_AVX512_MOVZXC
}


# EMITTING VMOVW (VMOVW-128-5)
{
ICLASS:      VMOVW
CPL:         3
CATEGORY:    DATAXFER
EXTENSION:   AVX512EVEX
ISA_SET:     AVX512_MOVZXC_128
EXCEPTIONS:  AVX512-E9NF
REAL_OPCODE: Y
PATTERN:     EVV 0x6E VF3 MAP5 MOD[0b11] MOD=3 BCRC=0 UBIT=1 REG[rrr] RM[nnn] W0 VL128 NOEVSR ZEROING=0 MASK=0
OPERANDS:    REG0=XMM_R3():w:dq:u16 REG1=XMM_B3():r:dq:u16
IFORM:       VMOVW_XMMu16_XMMu16_AVX512_MOVZXC
}

{
ICLASS:      VMOVW
CPL:         3
CATEGORY:    DATAXFER
EXTENSION:   AVX512EVEX
ISA_SET:     AVX512_MOVZXC_128
EXCEPTIONS:  AVX512-E9NF
REAL_OPCODE: Y
ATTRIBUTES:  DISP8_TUPLE1 
PATTERN:     EVV 0x6E VF3 MAP5 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() BCRC=0 UBIT=1 W0 VL128 NOEVSR ZEROING=0 MASK=0 ESIZE_16_BITS() NELEM_TUPLE1()
OPERANDS:    REG0=XMM_R3():w:dq:u16 MEM0:r:wrd:u16
IFORM:       VMOVW_XMMu16_MEMu16_AVX512_MOVZXC
}


# EMITTING VMOVW (VMOVW-128-6)
{
ICLASS:      VMOVW
CPL:         3
CATEGORY:    DATAXFER
EXTENSION:   AVX512EVEX
ISA_SET:     AVX512_MOVZXC_128
EXCEPTIONS:  AVX512-E9NF
REAL_OPCODE: Y
PATTERN:     EVV 0x7E VF3 MAP5 MOD[0b11] MOD=3 BCRC=0 UBIT=1 REG[rrr] RM[nnn] W0 VL128 NOEVSR ZEROING=0 MASK=0
OPERANDS:    REG0=XMM_B3():w:dq:u16 REG1=XMM_R3():r:dq:u16
IFORM:       VMOVW_XMMu16_XMMu16_AVX512_MOVZXC
}

{
ICLASS:      VMOVW
CPL:         3
CATEGORY:    DATAXFER
EXTENSION:   AVX512EVEX
ISA_SET:     AVX512_MOVZXC_128
EXCEPTIONS:  AVX512-E9NF
REAL_OPCODE: Y
ATTRIBUTES:  DISP8_TUPLE1 
PATTERN:     EVV 0x7E VF3 MAP5 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() BCRC=0 UBIT=1 W0 VL128 NOEVSR ZEROING=0 MASK=0 ESIZE_16_BITS() NELEM_TUPLE1()
OPERANDS:    MEM0:w:wrd:u16 REG0=XMM_R3():r:dq:u16
IFORM:       VMOVW_MEMu16_XMMu16_AVX512_MOVZXC
}


